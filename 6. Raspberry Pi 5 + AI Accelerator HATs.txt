1. Example: Real-time Object Detection with Raspberry Pi 5 + Coral USB Accelerator (Python)

This code demonstrates how to capture video from a USB camera connected to a Raspberry Pi 5, perform real-time object detection using a Google Coral USB Accelerator, and display the results.

Prerequisites:

Raspberry Pi 5: With Raspberry Pi OS (64-bit recommended) installed and updated.
Google Coral USB Accelerator: Plugged into a USB 3.0 port on your Raspberry Pi 5 (blue port for optimal speed).
USB Camera: A connected USB webcam (usually /dev/video0).
Edge TPU Runtime and PyCoral:
Follow Google Coral's official installation guide for Debian-based systems. This involves adding the Coral APT repository and installing libedgetpu1-std and pycoral.
Bash

# Add Google Coral APT repository
echo "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt update

# Install Edge TPU runtime and pycoral library
sudo apt install libedgetpu1-std
pip3 install pycoral
pip3 install Pillow  # For image processing
OpenCV Python bindings:
Bash

pip3 install opencv-python
Edge TPU-compatible Object Detection Model: Download a pre-compiled object detection model designed for the Edge TPU (e.g., mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite).
You can find these in Google's Coral models repository.
Bash

mkdir -p ~/coral_models
cd ~/coral_models
wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/edgetpu/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite
wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/edgetpu/coco_labels.txt
Labels File: A .txt file containing the class labels (e.g., coco_labels.txt from the above download).
rpi5_coral_object_detection.py

Python

import numpy as np
import cv2
from pycoral.adapters import common
from pycoral.adapters import detect
from pycoral.utils.edgetpu import make_interpreter
import time
import os

# --- Configuration ---
# Adjust these paths to your downloaded model and labels
MODEL_PATH = os.path.expanduser("~/coral_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite")
LABELS_PATH = os.path.expanduser("~/coral_models/coco_labels.txt")

# Camera settings
CAMERA_INDEX = 0 # Usually 0 for the first USB camera
FRAME_WIDTH = 640
FRAME_HEIGHT = 480
CONFIDENCE_THRESHOLD = 0.5 # Minimum confidence to display a detection

# --- Load labels ---
def load_labels(path):
    with open(path, 'r') as f:
        return {i: line.strip() for i, line in enumerate(f.readlines())}

labels = load_labels(LABELS_PATH)

# --- Load the Edge TPU model ---
print(f"Loading model: {MODEL_PATH}")
try:
    interpreter = make_interpreter(MODEL_PATH)
    interpreter.allocate_tensors()
except Exception as e:
    print(f"Error loading Edge TPU model: {e}")
    print("Please ensure your Coral USB Accelerator is plugged in and the Edge TPU runtime is installed.")
    print("Also, verify the model path and that it's correctly compiled for Edge TPU.")
    exit(1)

# Get input size and type
input_details = interpreter.get_input_details()
input_height = input_details[0]['shape'][1]
input_width = input_details[0]['shape'][2]
input_type = input_details[0]['dtype']

print(f"Model input shape: ({input_height}, {input_width}), type: {input_type}")

# --- Initialize Camera ---
cap = cv2.VideoCapture(CAMERA_INDEX)
if not cap.isOpened():
    print(f"Error: Could not open camera at index {CAMERA_INDEX}")
    exit(1)

cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)

print(f"Camera opened: {FRAME_WIDTH}x{FRAME_HEIGHT}")

# --- Main loop for inference ---
print("Starting real-time object detection. Press 'q' to quit.")
try:
    while True:
        ret, frame = cap.read()
        if not ret:
            print("Failed to grab frame, exiting...")
            break

        # Preprocess frame for model input
        # Resize to model's expected input size
        # Note: OpenCV reads as BGR, models usually expect RGB. PyCoral handles this.
        # But if model expects specific scaling/normalization, apply here.
        resized_frame = cv2.resize(frame, (input_width, input_height), interpolation=cv2.INTER_AREA)

        # Set the input tensor
        common.set_input(interpreter, resized_frame)

        start_time = time.monotonic()
        interpreter.invoke() # Perform inference on Edge TPU
        end_time = time.monotonic()

        # Get results
        detections = detect.get_objects(interpreter, CONFIDENCE_THRESHOLD)

        # Draw bounding boxes
        frame_display = frame.copy()
        current_detections = 0
        for obj in detections:
            current_detections += 1
            bbox = obj.bbox
            x_min, y_min, x_max, y_max = int(bbox.xmin), int(bbox.ymin), int(bbox.xmax), int(bbox.ymax)

            # Scale bounding box coordinates back to original frame size
            x_scale = FRAME_WIDTH / input_width
            y_scale = FRAME_HEIGHT / input_height

            x_min_orig = int(x_min * x_scale)
            y_min_orig = int(y_min * y_scale)
            x_max_orig = int(x_max * x_scale)
            y_max_orig = int(y_max * y_scale)

            color = (0, 255, 0) # Green BGR
            cv2.rectangle(frame_display, (x_min_orig, y_min_orig), (x_max_orig, y_max_orig), color, 2)
            
            label_text = f"{labels.get(obj.id, 'Unknown')}: {obj.score:.2f}"
            cv2.putText(frame_display, label_text, (x_min_orig, y_min_orig - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)
        
        inference_time_ms = (end_time - start_time) * 1000
        fps = 1000 / inference_time_ms if inference_time_ms > 0 else 0

        info_text = f"FPS: {fps:.2f} | Detections: {current_detections} | Latency: {inference_time_ms:.2f}ms"
        cv2.putText(frame_display, info_text, (10, 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)


        cv2.imshow("Raspberry Pi 5 + Coral Edge AI", frame_display)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

except KeyboardInterrupt:
    print("\nExiting program due to user interrupt.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")
finally:
    if 'cap' in locals() and cap.isOpened():
        cap.release()
    cv2.destroyAllWindows()
    print("Camera released and windows closed.")

How to run this code on your Raspberry Pi 5:

SSH into your Raspberry Pi 5 or work directly on it.
Ensure all prerequisites are met (Raspberry Pi OS updated, Coral USB Accelerator plugged in, Edge TPU runtime/pycoral, OpenCV installed, model/labels downloaded).
Save the code above as rpi5_coral_object_detection.py.
Execute:
Bash

python3 rpi5_coral_object_detection.py
A window showing your camera feed with real-time object detections should appear on your Raspberry Pi's desktop (if connected to a display) or via VNC. Press 'q' to quit.