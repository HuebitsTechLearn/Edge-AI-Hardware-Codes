âš™ï¸ Step 1: Install LEIP SDK (Python CLI)
bash
Copy
Edit
pip install latentai-leip
If you're a registered LatentAI partner, you'll use their private SDK repo or prebuilt CLI .whl installer â€” request via latentai.com/contact

ğŸ“Š Step 2: Profile Your Model
âœ… Example: Profiling a MobileNetV2 ONNX Model
bash
Copy
Edit
leip profile \
  --model mobilenetv2.onnx \
  --data-dir ./calibration_data/ \
  --input-shape "1,3,224,224"
Youâ€™ll get:

Latency report

Memory usage

Operator fusion stats

Edge readiness score (ğŸ”¥)

ğŸ§¬ Step 3: Optimize and Quantize
âœ… Example: Static Quantization
bash
Copy
Edit
leip optimize \
  --model mobilenetv2.onnx \
  --quantization static \
  --target cortex-m \
  --data-dir ./calibration_data/ \
  --output optimized_model.onnx
âœ… For dynamic quantization:
bash
Copy
Edit
leip optimize \
  --model resnet18.onnx \
  --quantization dynamic \
  --target x86 \
  --output optimized_resnet18.onnx
ğŸš€ Step 4: Deploy to Edge Hardware
âœ… Deployment for Cortex-M (e.g., STM32, Arduino Portenta):
bash
Copy
Edit
leip deploy \
  --model optimized_model.onnx \
  --target cortex-m \
  --format c_array \
  --output-dir ./deploy_model/
Outputs a model.h and model.c for embedded deployment. Plug it into your embedded C++ project.

ğŸ§  Step 5: Inference Runtime (Python)
If you're testing optimized ONNX locally:

python
Copy
Edit
import onnxruntime as ort
import numpy as np

session = ort.InferenceSession("optimized_model.onnx")
input_name = session.get_inputs()[0].name

input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
result = session.run(None, {input_name: input_data})

print("Output:", result)
ğŸ“¦ Project Structure Example
bash
Copy
Edit
/project
 â”œâ”€â”€ model.onnx
 â”œâ”€â”€ calibration_data/
 â”œâ”€â”€ optimized_model.onnx
 â”œâ”€â”€ deploy_model/
 â”‚   â””â”€â”€ model.c
 â”‚   â””â”€â”€ model.h
 â”œâ”€â”€ profile_report.json
 â””â”€â”€ test_inference.py